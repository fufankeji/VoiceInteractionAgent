"""Qwen3-VL-Plus å¤šæ¨¡æ€æ¨ç†å°è£… (Realtime Backend)"""
from __future__ import annotations

import asyncio
import base64
import logging
import time
from typing import List, Literal, Optional, Dict, Any, Union

from openai import AsyncOpenAI

from app.core.config import get_settings
from app.services.emotion_engine import emotion_engine
from app.services.info_agent import get_info_agent
from app.utils.persona_prompt import build_static_persona_prompts
from app.utils.text_cleaner import strip_stage_directions

logger = logging.getLogger(__name__)

settings = get_settings()
client = AsyncOpenAI(api_key=settings.dashscope_api_key, base_url=str(settings.qwen_base_url))
BASE_SYSTEM_PROMPT = (
    "ä½ æ˜¯ä¸€ä¸ªå¼ºå¤§çš„å¤šæ¨¡æ€ AI åŠ©æ‰‹ï¼Œå…·å¤‡è§†è§‰ç†è§£ã€å¯¹è¯äº¤äº’å’Œä»»åŠ¡è§„åˆ’èƒ½åŠ›ã€‚è¯·ç®€æ´å‡†ç¡®åœ°å›ç­”ç”¨æˆ·é—®é¢˜ã€‚"
)
NO_STAGE_DIRECTION_PROMPT = (
    "ä¸ç”¨æˆ·å¯¹è¯æ—¶è¯·ä½¿ç”¨è‡ªç„¶å£è¯­ï¼Œä¸è¦ä½¿ç”¨èˆå°æŒ‡ä»¤ã€*åŠ¨ä½œ*ã€æ‹¬å·ä¸­çš„åŠ¨ä½œæå†™æˆ–æ—ç™½ï¼Œ"
    "åªç”¨ç›´æ¥çš„å¯¹è¯å¥å­è¡¨è¾¾æƒ…ç»ªã€‚"
)


class SessionContext:
    """ä¼šè¯ä¸Šä¸‹æ–‡ (ç®€åŒ–ç‰ˆ)"""
    def __init__(self):
        self.history = []


def _build_prompt(
    history: List[dict],
    user_text: str,
    images: Optional[List[str]] = None,
    persona_prompts: Optional[List[str]] = None,
    emotion_instruction: Optional[str] = None,
) -> List[dict]:
    """
    æ„å»ºå¤šæ¨¡æ€æç¤ºè¯
    
    Args:
        history: å†å²å¯¹è¯
        user_text: ç”¨æˆ·æ–‡æœ¬
        images: å›¾ç‰‡ URL æˆ– Base64 åˆ—è¡¨
        persona_prompts: è§’è‰²æ‰®æ¼”æç¤ºè¯åˆ—è¡¨
        emotion_instruction: å½“å‰å¿ƒæƒ…æŒ‡ä»¤
    
    Returns:
        OpenAI æ ¼å¼çš„æ¶ˆæ¯åˆ—è¡¨
    """
    system_prompt_blocks: List[str] = [BASE_SYSTEM_PROMPT]
    if persona_prompts:
        system_prompt_blocks.extend(persona_prompts)
    if emotion_instruction:
        system_prompt_blocks.append(emotion_instruction)
    system_prompt_blocks.append(NO_STAGE_DIRECTION_PROMPT)

    messages: List[dict] = [
        {
            "role": "system",
            "content": "\n\n".join(block.strip() for block in system_prompt_blocks if block.strip()),
        }
    ]
    
    # ä¿ç•™æœ€è¿‘6è½®å†å²
    for turn in history[-6:]:
        messages.append({"role": turn["role"], "content": turn["content"]})
    
    # æ„å»ºç”¨æˆ·æ¶ˆæ¯ (æ”¯æŒå¤šæ¨¡æ€)
    if images and len(images) > 0:
        # å¤šæ¨¡æ€æ¶ˆæ¯: æ–‡æœ¬ + å›¾åƒ
        content = [{"type": "text", "text": user_text}]
        
        for img in images:
            if img.startswith("http://") or img.startswith("https://"):
                # URL å›¾ç‰‡
                content.append({
                    "type": "image_url",
                    "image_url": {"url": img}
                })
            elif img.startswith("data:image"):
                # Data URL (Base64)
                content.append({
                    "type": "image_url",
                    "image_url": {"url": img}
                })
            else:
                # çº¯ Base64 (è¡¥å…… data URL å‰ç¼€)
                content.append({
                    "type": "image_url",
                    "image_url": {"url": f"data:image/jpeg;base64,{img}"}
                })
        
        messages.append({"role": "user", "content": content})
        logger.debug(f"[LLM] æ„å»ºå¤šæ¨¡æ€æç¤ºè¯: æ–‡æœ¬ + {len(images)} å¼ å›¾ç‰‡")
    else:
        # çº¯æ–‡æœ¬æ¶ˆæ¯
        messages.append({"role": "user", "content": user_text})
        logger.debug(f"[LLM] æ„å»ºæ–‡æœ¬æç¤ºè¯")
    
    logger.debug(f"[LLM] å†å²è½®æ•°: {len(history[-6:])}, æ€»æ¶ˆæ¯æ•°: {len(messages)}")
    return messages


async def chat(
    history: List[dict], 
    user_text: str,
    images: Optional[List[str]] = None,
    enable_tools: bool = False,
    stream: bool = False,
):
    """
    è°ƒç”¨ Qwen3-VL-Plus å®Œæˆå¤šæ¨¡æ€å¯¹è¯æ¨ç†
    
    Args:
        history: å†å²å¯¹è¯
        user_text: ç”¨æˆ·æ–‡æœ¬
        images: å›¾ç‰‡åˆ—è¡¨ (URL æˆ– Base64)
        enable_tools: æ˜¯å¦å¯ç”¨å·¥å…·è°ƒç”¨ (Agent èƒ½åŠ›)
        stream: æ˜¯å¦æµå¼è¿”å›
    
    Returns:
        å¦‚æœ stream=Trueï¼Œè¿”å›å¼‚æ­¥ç”Ÿæˆå™¨ AsyncGenerator[str, None]
        å¦‚æœ stream=Falseï¼Œè¿”å›å®Œæ•´æ–‡æœ¬ str
    """
    
    chat_start = time.perf_counter()

    persona_prompts = build_static_persona_prompts()
    if persona_prompts:
        logger.info("[LLM] ğŸ§¬ Persona prompts active: %d æ¡", len(persona_prompts))
    elif settings.persona_enabled:
        logger.warning("[LLM] âš ï¸ Persona å·²å¯ç”¨ä½†æœªç”Ÿæˆæç¤ºè¯, è¯·æ£€æŸ¥é…ç½®å†…å®¹")
    else:
        logger.debug("[LLM] Persona æç¤ºè¯å·²ç¦ç”¨")
    emotion_instruction: Optional[str] = None
    if settings.emotion_enabled:
        try:
            emotion_instruction = await emotion_engine.instruction_for(user_text)
        except Exception as exc:
            logger.warning("[Emotion] æŒ‡ä»¤ç”Ÿæˆå¤±è´¥: %s", exc)
    if emotion_instruction:
        logger.info("[LLM] ğŸ’“ Emotion instruction injected")

    messages = _build_prompt(
        history,
        user_text,
        images,
        persona_prompts=persona_prompts,
        emotion_instruction=emotion_instruction,
    )
    payload = {
        "model": settings.default_llm_model,
        "messages": messages,
        "temperature": 0.7,
        "stream": stream,
    }
    
    # TODO: åç»­å¯æ‰©å±•å·¥å…·è°ƒç”¨
    # if enable_tools:
    #     payload["tools"] = [...]
    
    # åˆ†æ®µæ—¥å¿—ï¼Œçªå‡ºâ€œå†å²å¯¹è¯â€ä¸â€œå½“å‰è¯·æ±‚â€ï¼Œå†å²éƒ¨åˆ†ç”¨å—çŠ¶æ ‡è®°æ˜¾ç¤ºï¼ˆå•æ¡ INFO é¿å…é‡å¤æ—¶é—´æˆ³ï¼‰
    logger.info(f"[LLM] è°ƒç”¨æ¨¡å‹: {settings.default_llm_model}, temperature=0.7, stream={stream}, å¤šæ¨¡æ€={bool(images)}")
    logger.info("[LLM] ğŸ“¤ å‘é€ç»™å¤§æ¨¡å‹ (åˆ†æ®µå±•ç¤º)")
    # system
    logger.info("[LLM]   [system] %s", messages[0]["content"])
    # history (ä»…é¢„è§ˆï¼Œå®Œæ•´å†…å®¹å†™åˆ° DEBUG)
    history_slice = history[-6:]
    if history_slice:
        preview_lines = []
        tail_count = min(3, len(history_slice))
        for msg in history_slice[-tail_count:]:
            snippet = str(msg.get("content", ""))[:120]
            tail = "..." if len(snippet) == 120 else ""
            preview_lines.append(f"- role={msg.get('role')}: {snippet}{tail}")
        block = "\n".join(preview_lines) if preview_lines else "æ— "
        logger.info("[LLM] ===== å†å²å¯¹è¯é¢„è§ˆ =====")
        logger.info("[LLM]   å·²åŠ è½½å†å²æ¡æ•°: %dï¼Œä»¥ä¸‹ä»…å±•ç¤ºæœ«å°¾ %d æ¡", len(history_slice), tail_count)
        logger.info("[LLM] %s", block)
        logger.info("[LLM] ===== é¢„è§ˆç»“æŸ =====")
        # è¯¦ç»†å†…å®¹è½¬åˆ° DEBUGï¼Œé¿å…å¹²æ‰°å½“å‰äº¤äº’é˜…è¯»
        for i, msg in enumerate(history_slice, 1):
            logger.debug("[LLM][history-full] (%d/%d) role=%s content=%s", i, len(history_slice), msg.get("role"), msg.get("content"))
    else:
        logger.info("[LLM] ===== å†å²å¯¹è¯é¢„è§ˆ =====")
        logger.info("[LLM]   å·²åŠ è½½å†å²æ¡æ•°: 0")
        logger.info("[LLM]   æ— ")
        logger.info("[LLM] ===== é¢„è§ˆç»“æŸ =====")
    # current user (åŒ…å«å¤šæ¨¡æ€ä¿¡æ¯)
    last_msg = messages[-1]
    if isinstance(last_msg.get("content"), list):
        text_parts = [item.get("text", "") for item in last_msg["content"] if item.get("type") == "text"]
        image_count = sum(1 for item in last_msg["content"] if item.get("type") == "image_url")
        logger.info("[LLM]   [user] text=%s | images=%d", " ".join(text_parts), image_count)
    else:
        logger.info("[LLM]   [user] %s", last_msg.get("content"))

    # åªæŒ‰è°ƒç”¨æ–¹å¼€å…³å†³å®šæ˜¯å¦èµ° Agentï¼Œé¿å…å¤šæ¨¡æ€å›¾ç‰‡è¢« Agent æ‹¦æˆª
    use_agent = enable_tools
    if use_agent:
        agent_instance = get_info_agent()
        if agent_instance:
            try:
                agent_text = await agent_instance.arun(history, user_text)
                cleaned_agent_text = strip_stage_directions(agent_text)
                logger.info("[LLM] ğŸ¤– Agent è¿”å›å†…å®¹:")
                logger.info(f"[LLM]   {cleaned_agent_text}")
                logger.info(f"[LLM] æ¨ç†æˆåŠŸ(Agent),è¿”å›é•¿åº¦: {len(cleaned_agent_text)} å­—ç¬¦")
                return cleaned_agent_text
            except Exception as exc:
                logger.warning("[LLM] Agent æ‰§è¡Œå¤±è´¥ï¼Œå›é€€åˆ°åŸºç¡€ LLM: %s", exc)
    
    try:
        if stream:
            # æµå¼æ¨¡å¼ï¼šè¿”å›å¼‚æ­¥ç”Ÿæˆå™¨
            return _stream_chat(payload, start_time=chat_start)
        else:
            # éæµå¼æ¨¡å¼ï¼šè¿”å›å®Œæ•´æ–‡æœ¬
            response = await client.chat.completions.create(**payload)
            content = response.choices[0].message.content  # type: ignore[index]
            
            if isinstance(content, list):
                # OpenAI SDK å¯èƒ½è¿”å›å¯Œæ–‡æœ¬æ•°ç»„,è¿™é‡Œå–æ–‡æœ¬ç‰‡æ®µå¹¶æ‹¼æ¥
                result = "".join(block.get("text", "") for block in content)
                logger.debug(f"[LLM] è¿”å›å¯Œæ–‡æœ¬æ•°ç»„,æ‹¼æ¥åé•¿åº¦: {len(result)}")
            else:
                result = content or ""
            
            cleaned_result = strip_stage_directions(result)
            if cleaned_result != result:
                logger.info("[LLM] ğŸ§¹ Stage directions removed (%d -> %d chars)", len(result), len(cleaned_result))
            logger.info(f"[LLM] ğŸ“¥ å¤§æ¨¡å‹è¿”å›çš„å®Œæ•´å†…å®¹:")
            logger.info(f"[LLM]   {cleaned_result}")
            logger.info(f"[LLM] æ¨ç†æˆåŠŸ,è¿”å›é•¿åº¦: {len(cleaned_result)} å­—ç¬¦")
            
            # è®°å½• token ä½¿ç”¨æƒ…å†µ
            logger.info(f"[LLM] ğŸ“¥ å¤§æ¨¡å‹è¿”å›çš„å®Œæ•´å†…å®¹:")
            logger.info(f"[LLM]   {cleaned_result}")

            if hasattr(response, 'usage') and response.usage:
                logger.info(f"[LLM] ğŸ’° Token ä½¿ç”¨: prompt={response.usage.prompt_tokens}, "
                           f"completion={response.usage.completion_tokens}, "
                           f"total={response.usage.total_tokens}")
            elapsed = time.perf_counter() - chat_start
            logger.info(f"[LLM] â±ï¸ æ¨ç†è€—æ—¶: {elapsed:.2f}s")
            
            return cleaned_result
        
    except Exception as exc:
        logger.exception(f"[LLM] è°ƒç”¨å¤±è´¥: {exc}")
        raise


async def _stream_chat(payload: dict, start_time: Optional[float] = None):
    """
    æµå¼èŠå¤©çš„å¼‚æ­¥ç”Ÿæˆå™¨
    
    Yields:
        æ¯æ¬¡ç”Ÿæˆçš„æ–‡æœ¬ç‰‡æ®µ
    """
    logger.info(f"[LLM] ğŸŒŠ å¼€å§‹æµå¼æ¨ç†")
    full_text = ""
    stream_start = start_time if start_time is not None else time.perf_counter()
    
    try:
        response = await client.chat.completions.create(**payload)
        
        async for chunk in response:
            if chunk.choices and len(chunk.choices) > 0:
                delta = chunk.choices[0].delta
                if hasattr(delta, 'content') and delta.content:
                    content = delta.content
                    full_text += content
                    logger.debug(f"[LLM] ğŸ“¤ æµå¼ç‰‡æ®µ: {content}")
                    yield content
        
        elapsed = time.perf_counter() - stream_start
        logger.info(f"[LLM] ğŸŒŠ æµå¼æ¨ç†å®Œæˆ,æ€»é•¿åº¦: {len(full_text)} å­—ç¬¦,è€—æ—¶ {elapsed:.2f}s")
        
    except Exception as exc:
        logger.exception(f"[LLM] æµå¼æ¨ç†å¤±è´¥: {exc}")
        raise
